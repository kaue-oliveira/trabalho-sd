from fastapi import FastAPI, Request
import requests
import json
import os
import logging
import time

# Configurar logging com UTF-8
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

app = FastAPI(title="Ollama Decision Service")

OLLAMA_API_URL = os.getenv("OLLAMA_API_URL", "http://ollama:11434/api/generate")
MODEL = os.getenv("MODEL", "mistral")

@app.on_event("startup")
async def warmup_model():
    """
    Pr√©-carrega o modelo Ollama no startup para evitar timeout na primeira requisi√ß√£o
    """
    logger.info(f"Iniciando pr√©-carregamento do modelo {MODEL}...")
    try:
        warmup_prompt = "Teste de inicializa√ß√£o"
        response = requests.post(
            OLLAMA_API_URL,
            json={
                "model": MODEL,
                "prompt": warmup_prompt,
                "stream": False
            },
            timeout=90
        )
        if response.status_code == 200:
            logger.info(f"‚úÖ Modelo {MODEL} pr√©-carregado com sucesso!")
        else:
            logger.warning(f"‚ö†Ô∏è  Falha ao pr√©-carregar modelo: {response.status_code}")
    except Exception as e:
        logger.error(f"‚ùå Erro ao pr√©-carregar modelo: {e}")

@app.get("/")
def root():
    return {"service": "ollama_service", "status": "ok", "model": MODEL}

@app.get("/health")
def health():
    return {"status": "healthy", "model": MODEL}

@app.post("/generate")
async def generate_recommendation(request: Request):
    """
    Gera decis√£o de venda baseada em clima, pre√ßo e relat√≥rios t√©cnicos
    Modelo j√° est√° pr√©-carregado, ent√£o a resposta √© mais r√°pida
    """
    data = await request.json()
    clima = data.get("clima", {})
    preco = data.get("preco", {})
    relatorios = data.get("relatorios", [])
    localidade = data.get("localidade", "")
    data_colheita = data.get("data_colheita", "")

    # Montar contexto dos relat√≥rios
    contexto_relatorios = "\n".join([
        f"- {r.get('text', '')[:200]}..." 
        for r in relatorios[:3]
    ])

    # Montar prompt com encoding UTF-8 expl√≠cito
    clima_str = json.dumps(clima, indent=2, ensure_ascii=False)
    preco_str = json.dumps(preco, indent=2, ensure_ascii=False)
    
    prompt = f"""Voc√™ √© um assistente especialista em caf√©. Analise os dados abaixo e recomende se o produtor deve VENDER, AGUARDAR ou VENDER_PARCIALMENTE o caf√©.

Localidade: {localidade}
Data de Colheita: {data_colheita}

Clima: {clima_str}

Pre√ßo: {preco_str}

Contexto de relat√≥rios t√©cnicos:
{contexto_relatorios}

Forne√ßa sua resposta APENAS em JSON v√°lido com os campos:
{{"decision": "vender|aguardar|vender_parcialmente", "explanation": "explica√ß√£o detalhada em at√© 200 palavras com acentua√ß√£o correta em portugu√™s"}}
"""

    try:
        logger.info(f"ü§ñ Gerando decis√£o para {localidade}...")
        
        # Chamar Ollama com streaming desabilitado
        # Timeout de 90s √© suficiente para phi3:mini (responde em ~10-20s)
        start = time.perf_counter()
        response = requests.post(
            OLLAMA_API_URL,
            json={
                "model": MODEL,
                "prompt": prompt,
                "stream": False,
                "format": "json"
            },
            timeout=90,
            headers={"Content-Type": "application/json; charset=utf-8"}
        )
        duration = time.perf_counter() - start
        response.raise_for_status()
        
        # Garantir encoding UTF-8 na resposta
        response.encoding = 'utf-8'
        response_data = response.json()
        response_text = response_data.get("response", "{}")
        
        logger.info(f"‚úÖ Decis√£o gerada com sucesso (tempo={duration:.3f}s)")
        
        # Tentar parsear a resposta JSON do modelo
        try:
            decision_data = json.loads(response_text)
            return {
                "decision": decision_data.get("decision", "aguardar").lower(),
                "explanation": decision_data.get("explanation", "Decis√£o gerada pelo modelo de IA"),
                "ollama_time_seconds": round(duration, 3)
            }
        except json.JSONDecodeError:
            logger.warning(f"Resposta do modelo n√£o √© JSON v√°lido")
            # Se n√£o conseguir parsear, retornar resposta como texto
            return {
                "decision": "aguardar",
                "explanation": response_text[:500] if response_text else "N√£o foi poss√≠vel gerar recomenda√ß√£o.",
                "ollama_time_seconds": round(duration, 3)
            }
            
    except requests.exceptions.Timeout:
        logger.error("Timeout ao gerar decis√£o")
        return {
            "decision": "aguardar",
            "explanation": "Timeout ao gerar decis√£o. Por favor, tente novamente.",
            "ollama_time_seconds": None
        }
    except Exception as e:
        logger.error(f"Erro ao gerar decis√£o: {e}")
        return {
            "decision": "aguardar",
            "explanation": f"Erro ao gerar decis√£o: {str(e)}",
            "ollama_time_seconds": None
        }